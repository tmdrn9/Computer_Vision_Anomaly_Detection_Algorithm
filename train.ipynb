{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Computer Vision 이상치 탐지 알고리즘 경진대회**\n",
        "\n",
        "---\n",
        "\n",
        "start : 220401\n",
        "\n",
        "end : 220513\n"
      ],
      "metadata": {
        "id": "GQz_FSZmbgjr"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gLnP7Yp2M2x7"
      },
      "source": [
        "# 라이브러리 불러오기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-zwB1emC39P1"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "import torch\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "from glob import glob\n",
        "import cv2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import PIL \n",
        "from torch import nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets, transforms\n",
        "import torchvision as tv\n",
        "from PIL import Image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mEQ2jFXo7hzu"
      },
      "source": [
        "# 환경설정"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "beMLRmMK5ZA1"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount._DEBUG = True\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BElO2hfb6va2"
      },
      "outputs": [],
      "source": [
        "os.chdir('./drive/MyDrive/이상치/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "BCAdaZ3O39K0",
        "outputId": "4c089323-0040-4d44-826d-1fe9b1848de0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cuda'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "device = 'cpu'\n",
        "if torch.cuda.is_available():\n",
        "    device = 'cuda'\n",
        "    \n",
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xtN7vi_m6NDA"
      },
      "source": [
        "# 데이터 로드"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H-TsLo2f6yk0"
      },
      "outputs": [],
      "source": [
        "# %%time\n",
        "# !unzip \"./open.zip\" -d \"/content/drive/MyDrive/Dacon/Computer Vision 이상치 탐지 알고리즘 경진대회/data\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "910_ooglOC_M"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "!unzip \"./data/train.zip\" -d \"/content/drive/MyDrive/이상치/data\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d74MZWzDOQPR"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "!unzip \"./data/test.zip\" -d \"/content/drive/MyDrive/이상치/data\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U8--KhQV39G2"
      },
      "outputs": [],
      "source": [
        "train_x = sorted(glob('./data/train/*.png'))\n",
        "test = sorted(glob('./data/test/*.png'))\n",
        "train_csv = pd.read_csv(\"./data/train_df.csv\")\n",
        "train_label = train_csv[\"label\"]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "label_unique = sorted(np.unique(train_label))\n",
        "\n",
        "label_unique_dir = {key:value for key,value in zip(label_unique, range(len(label_unique)))}\n",
        "\n",
        "train_Y = [label_unique_dir[k] for k in train_label]"
      ],
      "metadata": {
        "id": "O5XBwwTZDrKk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label_unique_dir"
      ],
      "metadata": {
        "id": "aW3iTjTxBczg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 데이터 증강"
      ],
      "metadata": {
        "id": "8qbbQ-It-mN6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install albumentations==0.4.6"
      ],
      "metadata": {
        "id": "UtO-9iA9-jbj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import albumentations\n",
        "import albumentations.pytorch"
      ],
      "metadata": {
        "id": "LT1HSbUB-PUb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "aug = albumentations.Compose([\n",
        "      albumentations.Resize(224, 224),\n",
        "      albumentations.HorizontalFlip(),\n",
        "      albumentations.VerticalFlip(),\n",
        "      albumentations.OneOf([\n",
        "                          albumentations.Rotate(),\n",
        "                          albumentations.ShiftScaleRotate()\n",
        " \n",
        "      ], p=1),\n",
        "      albumentations.augmentations.transforms.Normalize(mean=(0.5,), std=(0.5,), p=1.0),\n",
        "      albumentations.pytorch.transforms.ToTensorV2(p=1.0)\n",
        "      ])\n",
        "aug2 = albumentations.Compose([\n",
        "      albumentations.Resize(224, 224),\n",
        "      albumentations.Rotate(),\n",
        "      albumentations.augmentations.transforms.Normalize(mean=(0.5,), std=(0.5,), p=1.0),\n",
        "      albumentations.pytorch.transforms.ToTensorV2(p=1.0)\n",
        "      ])"
      ],
      "metadata": {
        "id": "V04945UFShSB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pUWpkidO5Ada"
      },
      "source": [
        "# 데이터셋과 데이터로더"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gps9L2xmTQRz"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train_x, val_x, train_y, val_y = train_test_split(train_x, train_Y, stratify=train_Y, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MyDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, x_dir, y_dir,transform=None,transform2=None):\n",
        "        super().__init__()\n",
        "        self.transforms = transform\n",
        "        self.transforms2 = transform2\n",
        "        self.x_img = x_dir\n",
        "        self.y = y_dir   \n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.x_img)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x_img = self.x_img[idx]\n",
        "        y = self.y[idx]\n",
        "\n",
        "        x_img = cv2.imread(x_img)\n",
        "        x_img = cv2.cvtColor(x_img, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        if self.transforms2 :\n",
        "            if 42<=y<=46:\n",
        "                augmented = self.transforms2(image=x_img)\n",
        "                x_img = augmented['image']\n",
        "            else:\n",
        "                augmented = self.transforms(image=x_img)\n",
        "                x_img = augmented['image']\n",
        "        else:\n",
        "            augmented = self.transforms(image=x_img)\n",
        "            x_img = augmented['image']\n",
        "\n",
        "\n",
        "        return x_img, y"
      ],
      "metadata": {
        "id": "_wvy_bZK_Jj4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r4f1-ecCBK6G"
      },
      "outputs": [],
      "source": [
        "train_dataset = MyDataset(train_x,train_y,aug,aug2)\n",
        "valid_dataset = MyDataset(val_x,val_y,aug,aug2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jCtukzaXn4tb",
        "outputId": "631dfa1c-2be2-435f-8f00-1118a16e9fd0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 4.8602,  4.4184,  4.4184,  0.2325,  6.9432,  8.1004,  8.1004,  6.9432,\n",
              "         9.7205,  0.2170,  8.1004,  9.7205,  9.7205,  4.0502,  4.4184,  0.2219,\n",
              "         4.4184,  4.0502,  4.8602,  4.8602,  5.4003,  0.1736,  5.4003,  5.4003,\n",
              "         4.8602,  8.1004,  8.1004,  8.1004,  0.1841,  8.1004,  8.1004,  5.4003,\n",
              "         5.4003,  0.1243,  5.4003,  5.4003,  4.8602,  4.8602,  5.4003,  4.8602,\n",
              "         0.1984,  5.4003,  3.7386,  4.4184,  4.0502,  0.2209,  4.0502,  3.7386,\n",
              "         5.4003,  4.4184,  3.7386,  4.8602,  0.1820,  9.7205,  4.0502,  0.1519,\n",
              "         4.0502,  4.0502,  3.7386,  4.0502,  4.0502,  5.4003,  5.4003,  0.2113,\n",
              "         6.0753,  5.4003,  6.0753,  3.2402,  0.8100,  9.7205,  9.7205,  9.7205,\n",
              "         0.2282,  9.7205, 12.1506,  8.1004,  0.1968,  9.7205,  9.7205,  4.4184,\n",
              "         4.8602,  6.0753,  5.4003,  6.0753,  0.2025,  5.4003,  5.4003,  6.0753],\n",
              "       device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "class_weights= torch.FloatTensor(compute_class_weight(class_weight = \"balanced\" , classes=list(range(88)), y = train_Y)).to(device)\n",
        "class_weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RdUBwU62-z9m"
      },
      "outputs": [],
      "source": [
        "batch_size=56\n",
        "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True,num_workers=4,pin_memory =False)\n",
        "valid_dataloader = torch.utils.data.DataLoader(valid_dataset, batch_size=batch_size, shuffle=True,num_workers=4,pin_memory =False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iZyMUinnfZbK"
      },
      "outputs": [],
      "source": [
        "#input과 label 확인\n",
        "images,labels = next(iter(train_dataloader))\n",
        "print(images.shape)\n",
        "print(labels.shape)\n",
        "print(labels[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dIRvokJRc_ov"
      },
      "source": [
        "# 모델 정의"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install timm\n",
        "import timm"
      ],
      "metadata": {
        "id": "RawPwu8aywOH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install efficientnet_pytorch\n",
        "from efficientnet_pytorch import EfficientNet"
      ],
      "metadata": {
        "id": "_EgXk2APRH6u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#모델확인\n",
        "avail_pretrained_models = timm.list_models(pretrained=True)\n",
        "len(avail_pretrained_models), avail_pretrained_models[:]"
      ],
      "metadata": {
        "id": "fLIvjkmxyqjR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#모델 불러오기\n",
        "model = timm.create_model('swin_tiny_patch4_window7_224',pretrained=True,num_classes=88,in_chans=3)"
      ],
      "metadata": {
        "id": "DdB2INyHy0X-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sum([param.nelement() for param in model.parameters()])"
      ],
      "metadata": {
        "id": "mjt1UwIReTa2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gNd-_sdo81Uv"
      },
      "source": [
        "# 학습 파라미터 설정"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9zITrZQMcJqM"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import f1_score, accuracy_score\n",
        "def score_function(real, pred):\n",
        "    score = f1_score(real, pred, average=\"macro\")\n",
        "    return score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "alzNWmNEM_H3"
      },
      "outputs": [],
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(),lr=0.001)\n",
        "scheduler=optim.lr_scheduler.ReduceLROnPlateau(optimizer,mode='min',patience=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 학습"
      ],
      "metadata": {
        "id": "AU50oWCjGHqK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# number of epochs to train the model\n",
        "n_epochs = 100\n",
        "valid_loss_min = np.inf # track change in validation loss 100epoch\n",
        "\n",
        "# keep track of training and validation loss\n",
        "train_loss = torch.zeros(n_epochs)\n",
        "valid_loss = torch.zeros(n_epochs)\n",
        "\n",
        "train_F1 = torch.zeros(n_epochs)\n",
        "valid_F1 = torch.zeros(n_epochs)\n",
        "model.to(device)\n",
        "\n",
        "for e in range(0, n_epochs):\n",
        "\n",
        "   \n",
        "    ###################\n",
        "    # train the model #\n",
        "    ###################\n",
        "    model.train()\n",
        "    for data, labels in tqdm(train_dataloader):\n",
        "        # move tensors to GPU if CUDA is available\n",
        "        data, labels = data.to(device), labels.to(device)\n",
        "        # forward pass: compute predicted outputs by passing inputs to the model\n",
        "        logits = model(data)\n",
        "        # calculate the batch loss\n",
        "        loss = criterion(logits, labels)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        # backward pass: compute gradient of the loss with respect to model parameters\n",
        "        loss.backward()\n",
        "        # perform a single optimization step (parameter update)\n",
        "        optimizer.step()\n",
        "        # update training loss\n",
        "        train_loss[e] += loss.item()\n",
        "        # update training score\n",
        "        logits=logits.argmax(1).detach().cpu().numpy().tolist()\n",
        "        labels=labels.detach().cpu().numpy().tolist()\n",
        "\n",
        "        train_F1[e] += score_function(labels,logits)\n",
        "\n",
        "    train_loss[e] /= len(train_dataloader)\n",
        "    train_F1[e] /= len(train_dataloader)\n",
        "        \n",
        "        \n",
        "    ######################    \n",
        "    # validate the model #\n",
        "    ######################\n",
        "    with torch.no_grad(): \n",
        "        model.eval()\n",
        "        for data, labels in tqdm(valid_dataloader):\n",
        "            # move tensors to GPU if CUDA is available\n",
        "            data, labels = data.to(device), labels.to(device)\n",
        "            # forward pass: compute predicted outputs by passing inputs to the model\n",
        "            logits = model(data)\n",
        "            # calculate the batch loss\n",
        "            loss = criterion(logits, labels)\n",
        "            # update average validation loss \n",
        "            valid_loss[e] += loss.item()\n",
        "            # update training score\n",
        "            logits=logits.argmax(1).detach().cpu().numpy().tolist()\n",
        "            labels=labels.detach().cpu().numpy().tolist()\n",
        "            valid_F1[e] += score_function(labels,logits)\n",
        "            \n",
        "    \n",
        "    # calculate average losses\n",
        "    valid_loss[e] /= len(valid_dataloader)\n",
        "    valid_F1[e] /= len(valid_dataloader)\n",
        "    \n",
        "    scheduler.step(valid_loss[e])    \n",
        "    # print training/validation statistics \n",
        "    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
        "        e, train_loss[e], valid_loss[e]))\n",
        "    \n",
        "    # print training/validation statistics \n",
        "    print('Epoch: {} \\tTraining accuracy: {:.6f} \\tValidation accuracy: {:.6f}'.format(\n",
        "        e, train_F1[e], valid_F1[e]))\n",
        "    \n",
        "    # save model if validation loss has decreased\n",
        "    if valid_loss[e] <= valid_loss_min:\n",
        "        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
        "        valid_loss_min,\n",
        "        valid_loss[e]))\n",
        "        torch.save(model.state_dict(), 'swin_tiny_patch4_window7_224.pt')\n",
        "        valid_loss_min = valid_loss[e]"
      ],
      "metadata": {
        "id": "zrb4xEPnGLNq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WA3zCOu30qxX"
      },
      "outputs": [],
      "source": [
        "plt.plot(train_loss)\n",
        "plt.plot(valid_loss) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "egYmWxyD0qtA"
      },
      "outputs": [],
      "source": [
        "plt.plot(train_F1)\n",
        "plt.plot(valid_F1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NHsrEjna5gkq"
      },
      "source": [
        "# 모델 로드 및 테스트 데이터셋과 데이터로더"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XU4wkstA0qo0"
      },
      "outputs": [],
      "source": [
        "model.load_state_dict(torch.load('swin_tiny_patch4_window7_224.pt'))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class TestDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, x_dir,transform=None):\n",
        "        super().__init__()\n",
        "        self.transforms = transform\n",
        "        self.x_img = x_dir \n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.x_img)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x_img = self.x_img[idx]\n",
        "\n",
        "        x_img = cv2.imread(x_img)\n",
        "        x_img = cv2.cvtColor(x_img, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        if self.transforms:\n",
        "            augmented = self.transforms(image=x_img)\n",
        "            x_img = augmented['image']\n",
        "\n",
        "        return x_img"
      ],
      "metadata": {
        "id": "Pix_uI2EQEKf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KN0wG3j4v1vK"
      },
      "outputs": [],
      "source": [
        "means=(0.5,)\n",
        "stds=(0.5,)\n",
        "testtransform = albumentations.Compose([\n",
        "      albumentations.Resize(300, 300),\n",
        "      albumentations.augmentations.transforms.Normalize(mean=means, std=stds, p=1.0),\n",
        "      albumentations.pytorch.transforms.ToTensorV2(p=1.0)\n",
        "      ])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size=64\n",
        "test_dataset = TestDataset(test,testtransform)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, shuffle=False, batch_size=batch_size)"
      ],
      "metadata": {
        "id": "OqPnLWUvr9BQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kK489J6SFrMz"
      },
      "source": [
        "# 추론 (TTA)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q13iZlFmFvoQ"
      },
      "outputs": [],
      "source": [
        "! pip install git+https://github.com/qubvel/ttach"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pnIJrzNFFx05"
      },
      "outputs": [],
      "source": [
        "import ttach as tta\n",
        "\n",
        "tta_transforms = tta.Compose(\n",
        "    [\n",
        "        tta.Rotate90(angles=[0, 90, 180, 270]),\n",
        "        tta.Multiply(factors=[0.9, 1, 1.1]),\n",
        "    ]\n",
        ")\n",
        "\n",
        "tta_model = tta.ClassificationTTAWrapper(model, tta_transforms)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bmiMRXaWFxxE"
      },
      "outputs": [],
      "source": [
        "tta_model.to(device)\n",
        "tta_model.eval()\n",
        "\n",
        "f_pred = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for x,_ in tqdm(test_loader):\n",
        "        x = x.to(device)\n",
        "        pred = tta_model(x)\n",
        "        f_pred.extend(pred.argmax(1).detach().cpu().numpy().tolist())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Z4Lat0d5qL_"
      },
      "source": [
        "# 제출물 생성"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RKY4nRqZJYzF"
      },
      "outputs": [],
      "source": [
        "label_decoder = {val:key for key, val in label_unique_dir.items()}\n",
        "\n",
        "f_result = [label_decoder[result] for result in f_pred]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-M1ziKV90xp-"
      },
      "outputs": [],
      "source": [
        "submission = pd.read_csv(\"./data/sample_submission.csv\")\n",
        "\n",
        "submission[\"label\"] = f_result\n",
        "\n",
        "submission"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "submission.to_csv(\"swin_tiny_patch4_window7_224.csv\", index = False)"
      ],
      "metadata": {
        "id": "dy2L4ILFQTsh"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "Computer_Vision_이상치_탐지_알고리즘_경진대회",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}